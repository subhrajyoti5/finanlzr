{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f306f3d",
   "metadata": {},
   "source": [
    "# Finanlzr Predictor Service on Colab\n",
    "\n",
    "This notebook runs the predictor Flask service for the Finanlzr app. It uses Prophet for forecasting when available, falling back to scikit-learn LinearRegression.\n",
    "\n",
    "Steps:\n",
    "1. Install dependencies\n",
    "2. Upload predictor.py (or copy from repo)\n",
    "3. Run the predictor service\n",
    "4. Expose with ngrok\n",
    "5. Use the public URL in your app's .env as PREDICTOR_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (TensorFlow, Flask, ngrok helper)\n",
    "!pip install --quiet tensorflow==2.12.0 flask flask-cors numpy pandas scikit-learn pyngrok psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a predictor.py that exposes a /predict endpoint using an LSTM model for short sequences.\n",
    "predictor = r'''\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import threading\n",
    "import hashlib\n",
    "import pickle\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Simple persistent cache on-disk to survive Colab cell restarts (small key/value store)\n",
    "CACHE_PATH = 'prediction_cache.pkl'\n",
    "try:\n",
    "    with open(CACHE_PATH, 'rb') as f:\n",
    "        prediction_cache = pickle.load(f)\n",
    "except Exception:\n",
    "    prediction_cache = {}\n",
    "\n",
    "\n",
    "def save_cache():\n",
    "    try:\n",
    "        with open(CACHE_PATH, 'wb') as f:\n",
    "            pickle.dump(prediction_cache, f)\n",
    "    except Exception as e:\n",
    "        print('Failed to save cache:', e)\n",
    "\n",
    "\n",
    "def cache_key(history, periods, mode):\n",
    "    # deterministic short key using sha1 of history and params\n",
    "    h = ','.join([str(float(x)) for x in history])\n",
    "    raw = f'{h}|{periods}|{mode}'\n",
    "    return hashlib.sha1(raw.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def build_lstm_model(input_len):\n",
    "    # small, efficient LSTM suitable for short series (e.g., 7-60 points)\n",
    "    model = Sequential([\n",
    "        LSTM(32, input_shape=(input_len, 1), return_sequences=False),\n",
    "        Dropout(0.08),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_sequences(history, lookback):\n",
    "    # normalize to percent changes to make model scale-invariant\n",
    "    arr = np.array(history, dtype=float)\n",
    "    if len(arr) < 2:\n",
    "        raise ValueError('Need at least 2 points')\n",
    "    pct = (arr[1:] - arr[:-1]) / (arr[:-1] + 1e-9)\n",
    "    # create sequences from pct changes; target is next pct change\n",
    "    X, y = [], []\n",
    "    for i in range(len(pct) - lookback):\n",
    "        X.append(pct[i:i+lookback])\n",
    "        y.append(pct[i+lookback])\n",
    "    if len(X) == 0:\n",
    "        # fallback: use last lookback as input if insufficient history\n",
    "        pad = np.zeros(lookback - len(pct)) if lookback > len(pct) else []\n",
    "        seq = np.concatenate([np.array(pct[-lookback:]) if len(pct) >= lookback else np.concatenate([np.zeros(max(0, lookback - len(pct))), pct])])\n",
    "        return np.array([seq]), np.array([0.0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def predict_lstm(history, periods=1, fast=True):\n",
    "    # Fast mode: tiny model trained for few epochs; accurate enough for short-term forecasts\n",
    "    hist = np.array(history, dtype=float)\n",
    "    n = len(hist)\n",
    "    lookback = 7 if n >= 14 else max(3, n//2)\n",
    "    # prepare sequences\n",
    "    try:\n",
    "        X, y = prepare_sequences(hist, lookback)\n",
    "    except Exception:\n",
    "        # fallback to linear regression prediction if we can't prepare sequences\n",
    "        return predict_linear(history, periods)\n",
    "    input_len = X.shape[1]\n",
    "    # reshape for LSTM: (samples, timesteps, features)\n",
    "    X_train = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    # build and train small model\n",
    "    model = build_lstm_model(input_len)\n",
    "    epochs = 20 if not fast else 6\n",
    "    batch = 8 if X_train.shape[0] >= 8 else 1\n",
    "    model.fit(X_train, y, epochs=epochs, batch_size=batch, verbose=0)\n",
    "    # iterative prediction in pct-change space\n",
    "    preds = []\n",
    "    last_window = X_train[-1].reshape(1, input_len, 1)\n",
    "    base_last = hist[-1]\n",
    "    for p in range(periods):\n",
    "        next_pct = float(model.predict(last_window, verbose=0).flatten()[0])\n",
    "        next_price = base_last * (1 + next_pct)\n",
    "        preds.append(float(next_price))\n",
    "        # roll for next iteration: compute next pct relative to predicted value\n",
    "        # convert new pct sequence for subsequent prediction\n",
    "        next_pct_norm = next_pct\n",
    "        window = last_window.flatten().tolist()[1:] + [next_pct_norm]\n",
    "        last_window = np.array(window).reshape(1, input_len, 1)\n",
    "        base_last = preds[-1]\n",
    "    return preds\n",
    "\n",
    "\n",
    "def predict_linear(history, periods=1):\n",
    "    # simple linear regression on raw values as fallback\n",
    "    try:\n",
    "        arr = np.array(history, dtype=float)\n",
    "        X = np.arange(len(arr)).reshape(-1, 1)\n",
    "        y = arr\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        xf = np.arange(len(arr), len(arr) + periods).reshape(-1, 1)\n",
    "        preds = model.predict(xf).tolist()\n",
    "        return [float(x) for x in preds]\n",
    "    except Exception as e:\n",
    "        # worst-case: repeat last value\n",
    "        return [float(history[-1])] * periods\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return jsonify({'ok': True, 'model': 'lstm', 'note': 'LSTM fast-mode enabled by default'})\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        body = request.get_json(force=True)\n",
    "        history = body.get('historical')\n",
    "        periods = int(body.get('periods', 1))\n",
    "        fast = bool(body.get('fast', True))\n",
    "        mode = 'lstm' if not body.get('force_linear') else 'linear'\n",
    "        if not history or not isinstance(history, list):\n",
    "            return jsonify({'error': 'provide `historical` as list of numbers'}), 400\n",
    "        history = [float(x) for x in history]\n",
    "        key = cache_key(history, periods, 'lstm' if not body.get('force_linear') else 'linear')\n",
    "        if key in prediction_cache:\n",
    "            return jsonify({'model': prediction_cache[key]['model'], 'predictions': prediction_cache[key]['predictions'], 'cached': True})\n",
    "        # If data is very short, use linear fallback quickly\n",
    "        if len(history) < 4 or body.get('force_linear'):\n",
    "            preds = predict_linear(history, periods)\n",
    "            prediction_cache[key] = {'model': 'linear', 'predictions': preds}\n",
    "            save_cache()\n",
    "            return jsonify({'model': 'linear', 'predictions': preds, 'cached': False})\n",
    "        # Otherwise try LSTM (fast mode by default), with try/except to fallback to linear\n",
    "        try:\n",
    "            preds = predict_lstm(history, periods=periods, fast=fast)\n",
    "            prediction_cache[key] = {'model': 'lstm', 'predictions': preds}\n",
    "            save_cache()\n",
    "            return jsonify({'model': 'lstm', 'predictions': preds, 'cached': False})\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            preds = predict_linear(history, periods)\n",
    "            prediction_cache[key] = {'model': 'linear', 'predictions': preds}\n",
    "            save_cache()\n",
    "            return jsonify({'model': 'linear', 'predictions': preds, 'cached': False})\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run Flask app (threaded) - Colab will keep it alive while cell is running\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)\n",
    "'''\n",
    "\n",
    "with open('predictor.py', 'w') as f:\n",
    "    f.write(predictor)\n",
    "\n",
    "print('Wrote predictor.py with LSTM-based predictor (fast-mode).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968af18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the predictor in background thread (so the notebook cell doesn't block)\n",
    "import subprocess, time, sys, os\n",
    "# Kill any existing predictor process (colab env might have one)\n",
    "try:\n",
    "    import psutil\n",
    "    for p in psutil.process_iter():\n",
    "        try:\n",
    "            cmd = ' '.join(p.cmdline() or [])\n",
    "            if 'predictor.py' in cmd:\n",
    "                p.kill()\n",
    "        except Exception:\n",
    "            continue\n",
    "except Exception:\n",
    "    pass\n",
    "proc = subprocess.Popen([sys.executable, 'predictor.py'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "time.sleep(2)\n",
    "print('Predictor launched (pid=', proc.pid, ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bff4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expose the local server using pyngrok\n",
    "from pyngrok import ngrok\n",
    "# Optionally set your ngrok auth token here if needed:\n",
    "# ngrok.set_auth_token('YOUR_NGROK_AUTH_TOKEN')\n",
    "public_url = ngrok.connect(5000)\n",
    "print('Public URL:', public_url)\n",
    "print('Use this URL as PREDICTOR_URL in your app\\'s .env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f872dd",
   "metadata": {},
   "source": [
    "## Testing the Predictor\n",
    "\n",
    "Test the service with a sample request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the predictor with a sample request\n",
    "import requests, time\n",
    "# public_url variable is returned by ngrok cell above (if run in same session)\n",
    "try:\n",
    "    url = public_url.public_url + '/predict'\n",
    "    data = {'historical': [100, 101, 102, 99, 105, 108, 110, 112], 'periods': 1}\n",
    "    r = requests.post(url, json=data, timeout=20)\n",
    "    print('Response:', r.json())\n",
    "except Exception as e:\n",
    "    print('Test failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc72982",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Keep the notebook running to keep the service alive.\n",
    "- If ngrok asks for auth, sign up at ngrok.com and set your token.\n",
    "- Update your app's .env with the ngrok URL and restart the app."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
